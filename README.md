# Knowledge Distillation
Experiments on knowledge distillation

## Experiemnts:
- Distil bert-base-uncased into a LSTM model

## Key Papers:
- Distilling the Knowledge in a Neural Network [https://arxiv.org/abs/1503.02531]
- Knowledge distillation: A good teacher is patient and consistent [https://arxiv.org/abs/2106.05237]


## Requirements:
- transformers
- torch
- pytorch_lightning
- wandb
